{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "127efe7e-ac4d-428c-a0bb-d90b2b11b0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: -4.27\n",
      "MSE: 643432406.75\n"
     ]
    }
   ],
   "source": [
    "### SET UP\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "### SET UP\n",
    "data = pd.read_csv('aes-2015-csv.csv')\n",
    "\n",
    "# Use this section to navigate to appropriate directory - it is assumed that CSV data is in your Downloads folder\n",
    "#os.chdir(\"Downloads\")  \n",
    "#print(os.getcwd())  \n",
    "\n",
    "### INITIAL THOUGHTS AND DATA SET OBSERVATIONS\n",
    "# Response Variable = Totak value which appears to be in millions of dollars\n",
    "# Data set is total income (plus breakdowns) for a variety of industries which look like they have a hierarchical nature to them (as per Industry_aggregation_NZSIOC)\n",
    "# Dataset is in an unusual format where each variable value is its own record in the DS - I will convert to tabular format for the sake of familiarity (I'm actually not sure if you can work with it straight like this)\n",
    "\n",
    "# Data set needs to be restructured to tabular format & values converted to numeric\n",
    "data['Value'] = pd.to_numeric(data['Value'], errors='coerce')\n",
    "data_tabular = data.pivot_table(index='Industry_name_NZSIOC', columns='Variable_name', values='Value', aggfunc='first')\n",
    "data_tabular.reset_index(inplace=True)\n",
    "\n",
    "# Make sure everything looks like it has converted properly\n",
    "#print(data_tabular.columns)\n",
    "#print(data_tabular['Total income'].mean())\n",
    "\n",
    "# Am just going to drop NaN values as I don't know anything about this data to choose the most appropriate imputation method\n",
    "data_cleaned = data_tabular.dropna()\n",
    "# Turns out that drops everything...\n",
    "missing_values = data_tabular.isnull().sum()\n",
    "#print(missing_values) \n",
    "\n",
    "sorted_columns = missing_values.sort_values()\n",
    "top_cols = sorted_columns.head(30).index\n",
    "data_top = data_tabular[top_cols]\n",
    "\n",
    "# Now we can drop NaN values without removing the whole dataset as we've already excluded sparse vars.\n",
    "data_cleaned_2 = data_top.dropna()\n",
    "numeric_data = data_cleaned_2.select_dtypes(include=[np.number])\n",
    "\n",
    "# Split into Response & Predictors\n",
    "X = numeric_data.drop(columns=['Total income'])\n",
    "y = numeric_data['Total income'] \n",
    "\n",
    "# Create a correlation matrix & drop predictors above 90% correlation\n",
    "threshold = 0.9\n",
    "correlation_matrix = X.corr()\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "X = X.drop(columns=to_drop)\n",
    "\n",
    "# Find 4 most correlated predictors to response var\n",
    "corr_with_y = X.corrwith(y)\n",
    "sorted_corr = corr_with_y.abs().sort_values(ascending=False)\n",
    "top_4_correlated = sorted_corr.head(4)\n",
    "top_4_variables = top_4_correlated.index\n",
    "X = X[top_4_variables]\n",
    "\n",
    "#################################################\n",
    "###   Thoughts on selected 4 response vars   ####\n",
    "#################################################\n",
    "# I've selected my 4 response variables based on the fact that they are the top 4 highest correlated to the response var \n",
    "# The threshold for multicollinearity is arbitrary right now & I have not been able to have a good look around the data given I've been told this should take me 3 hours.\n",
    "# The 4 predictors Closing Stocks, Current Ratio, Sales of Goods... and Quick Ratio have been the chosen 4 as they seem like a good place to start exploring\n",
    "\n",
    "# Next steps is to start modelling so we'd want to split into test and train datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "LRmod = LinearRegression()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "# model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"R2: {r2:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "\n",
    "### Scaled & Non scaled models produce virtually the same results.\n",
    "### The model is not good... I'm going to assume as I've been asked to keep it to 3 hours I shouldn't be going into refinements\n",
    "### ...or the test is supposed to be on data that is painful to work with and returns something disappointing haha\n",
    "### My gut feeling why this data isn't modelling well is because there is a lot of variation between industries. There probably wasn't enough data to look at each industry separately, but my thoughts are heading to an ensembe of separate models for each industry.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
